{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9a1aa2d-f63e-49ff-9d7e-e2f8d2986062",
   "metadata": {},
   "source": [
    "In this notebook, we'll convert some EPhys data from openephys(legacy) to the Neurodata Without Borders (.nwb) format.\n",
    "\n",
    "Use a virtual enviroment with the following dependencies:\n",
    "- neuroconv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c95cca-9c0c-4036-9ce4-cdd8f00e3e4e",
   "metadata": {},
   "source": [
    "Each experimental input is associated to an `Interface`. So each of your data files will be associated with an `Interface`. The metadata associated to each input depends on its `Interface' type, so you should make sure that you're using the right one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbe905f-6da5-4132-be71-d610b2ef4463",
   "metadata": {},
   "source": [
    "We'll first make a .nwb file containing only one input. This should clarify the basic idea. Then we'll combine a few inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905032d3-577d-47bd-a641-da02fab76228",
   "metadata": {},
   "source": [
    "# One input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ff253f-3fde-4a8f-9439-3abab3bd8dab",
   "metadata": {},
   "source": [
    "First, we need to import some useful tools from the `neuroconv` library. These deal with the structure of the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92d6ec58-8c7d-4b13-962e-0e08a6fd36b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuroconv.utils.dict import load_dict_from_file, dict_deep_update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d553d3-f605-4b9f-ab9b-9ab5f885e950",
   "metadata": {},
   "source": [
    "We'll now import the recording. This will be associated with an `Interface`. The correct `Interface` for a recording in the OpenEphys format is a `OpenEphysRecordingInterface`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac2ba03f-5396-40a0-9b3a-a21844e3e247",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuroconv.datainterfaces import OpenEphysRecordingInterface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fe91c4-d84e-4f5a-95b2-e87134cb8cc6",
   "metadata": {},
   "source": [
    "We point this `Interface` at our recording, by directing it to the directory that the recording is in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57390d95-1644-400f-ad2b-b3d8831757df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "folder_path = f\"somedata/stream/\"\n",
    "data_name = ''\n",
    "\n",
    "# Note: if there are multiple streams, like CH and ADC, you need to tell it which stream to use\n",
    "my_ephys_interface = OpenEphysRecordingInterface(folder_path=folder_path, stream_name=\"Signals CH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7be4593-c03a-47a5-b828-56adcc005e82",
   "metadata": {},
   "source": [
    "We've now created an `Interface` called `my_ephys_interface`. This has already taken a look at our data, made sure it can read it, and extracted any metadata it can. We can take a look at this metadata. It is stored in a `DeepDict` which is a custom `Dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eda601f9-3c40-447b-9ca0-82a09d7d3248",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chris/.my_venvs/SpikeDev/lib/python3.11/site-packages/neuroconv/datainterfaces/ecephys/openephys/openephyslegacydatainterface.py:76: UserWarning: The timestamp for starting time from openephys metadata is ambiguous ('12:38:29')! Only the date will be auto-populated in metadata. Please update the timestamp manually to record this value with the highest known temporal resolution.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "existing_data = my_ephys_interface.get_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e188818-9c3c-40f7-9ded-7b22e4bea04d",
   "metadata": {},
   "source": [
    "In my experience, it doesn't do a very good job of extracting metadata. There is a LOT more metadata we can input. All the options are stored in a **schema**: a schema tells you the structure of a database (or dictionary). We can check out the schema, though it is a bit intimidating to look at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ebad241-ce47-4607-b3b5-df5f72700fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_ephys_interface.get_metadata_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7df6831-5c53-41af-8069-096aef4edcf6",
   "metadata": {},
   "source": [
    "Gross. So that you don't have to use this, I've made a yaml from schema function. This takes in your interface and the path to where you want to save the metadata. There are two optional arguments: `descriptions = True` or `False` and `existing_data` which will input any existing data into the yaml file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f320eba-368a-444f-9f5b-1fc6eb6d4bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_yaml_from_schema(interface, metadata_path, descriptions=True,existing_data=None):\n",
    "    \n",
    "    the_schema = interface.get_metadata_schema()\n",
    "    \n",
    "    metadata_file = open(metadata_path, 'w')\n",
    "    \n",
    "    for level1key in list(the_schema['properties'].keys()):\n",
    "    \n",
    "        print(f\"{level1key}:\", file=metadata_file)\n",
    "    \n",
    "        level2keys = list(the_schema['properties'][level1key]['properties'].keys())\n",
    "        if 'definitions' in level2keys:\n",
    "            level2keys.remove('definitions')\n",
    "            \n",
    "        for level2key in level2keys:\n",
    "    \n",
    "            print( f\"  {level2key}: \", end=\"\", file=metadata_file)\n",
    "    \n",
    "            if existing_data is not None and level2key in existing_data[level1key] and type(existing_data[level1key][level2key]) == str:            \n",
    "                print(f\"{existing_data[level1key][level2key]}\" , end=\"\", file=metadata_file)\n",
    "                \n",
    "            if descriptions == True:\n",
    "                try:\n",
    "                    print(f\" # {the_schema['properties'][level1key]['properties'][level2key]['description']}\" , file=metadata_file, end=\"\")\n",
    "                except:\n",
    "                    if level1key != 'Ecephys':\n",
    "                        print(f\" # No description given\",  file=metadata_file, end=\"\")\n",
    "            \n",
    "            print(\"\", file=metadata_file)\n",
    "            \n",
    "            if the_schema['properties'][level1key]['properties'][level2key]['type'] == 'array':\n",
    "    \n",
    "                if '$ref' in the_schema['properties'][level1key]['properties'][level2key]['items']:\n",
    "                    print(f\"    - {{\\n\", end=\"\", file=metadata_file)\n",
    "                    for level3key in the_schema['properties']['Ecephys']['properties']['definitions'][level2key]['properties'].keys():\n",
    "                        \n",
    "                        print(f\"    {level3key}: ,\", file=metadata_file, end=\"\")\n",
    "                        #print(f\"{level1key}, {level2key}, {level3key}\", file=metadata_file, end=\"\")\n",
    "                                \n",
    "                        if descriptions == True:\n",
    "                            try:\n",
    "                                print(f\" # {the_schema['properties'][level1key]['properties']['definitions'][level2key]['properties'][level3key]['description']}\", file=metadata_file, end=\"\")\n",
    "                            except:\n",
    "                                print(f\" # No description given\", file=metadata_file, end=\"\")\n",
    "                        print(\"\", file=metadata_file)\n",
    "                    \n",
    "                    print(f\"}}\", file=metadata_file)\n",
    "                \n",
    "                else:\n",
    "                    print(f\"    -\", file=metadata_file)\n",
    "    \n",
    "            if level2key == 'ElectricalSeries':\n",
    "    \n",
    "                #print(\"\", file=metadata_file)\n",
    "               \n",
    "                for level3key in list(the_schema['properties'][level1key]['properties'][level2key]['properties'].keys()):\n",
    "                    print( f\"    {level3key}: \", end=\"\", file=metadata_file)\n",
    "    \n",
    "                    if existing_data is not None and level3key in existing_data[level1key][level2key] and type(existing_data[level1key][level2key][level3key]) == str:            \n",
    "                        print(f\"{existing_data[level1key][level2key][level3key]}\" ,end=\"\", file=metadata_file)\n",
    "                \n",
    "                    \n",
    "                    if descriptions == True:\n",
    "                        try:\n",
    "                            print(f\" # {the_schema['properties'][level1key]['properties'][level2key]['properties'][level3key]['description']}\", file=metadata_file, end=\"\")\n",
    "                        except:\n",
    "                            print(f\" # No description given\", file=metadata_file, end=\"\")\n",
    "                    \n",
    "                    print(\"\", file=metadata_file)\n",
    "                \n",
    "    metadata_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab608eb-b465-44fd-b431-42ab8c7df860",
   "metadata": {},
   "source": [
    "Below are a few examples. Go and explore the YAML files! Any simple text edit can open them.\n",
    "\n",
    "Note: you can also update the metadata directly in Python by editing the `metadata` dictionary. For more details, see the Neuroconv documentation (https://neuroconv.readthedocs.io/en/main/user_guide/nwbconverter.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ab4b44f-70d2-4b6c-b880-f11b3d1aa359",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_yaml_from_schema(my_ephys_interface, \"metadata_plain.yml\", descriptions=False)\n",
    "make_yaml_from_schema(my_ephys_interface, \"metadata_descriptions.yml\", descriptions=True)\n",
    "make_yaml_from_schema(my_ephys_interface, \"metadata_descriptions_withdata.yml\", descriptions=True, existing_data = existing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a935f49-69bc-412f-88aa-5c56b7456449",
   "metadata": {},
   "source": [
    "Our data folder is set. Our metadata has been filled out. We're ready to make a .nwb file! Note that some of the metadata is __required__: the .nwb file will not compile if you've not supplied it. If this error comes up, just go and update the .yml file.\n",
    "\n",
    "We use the `run_conversion` function to run the file conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a64ac21-2017-486d-b96d-b3196dace453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NWB file saved at nwbdata/harry.nwb!\n"
     ]
    }
   ],
   "source": [
    "nwbfile_path = \"nwbdata/harry.nwb\"  # Where to save the .nwb file\n",
    "metadata_path = \"metadata_updated.yml\"\n",
    "\n",
    "metadata_from_yaml = load_dict_from_file(file_path=metadata_path)\n",
    "\n",
    "if isinstance(metadata_from_yaml, dict):\n",
    "    my_ephys_interface.run_conversion(nwbfile_path=nwbfile_path, metadata=metadata_from_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb017ee-971f-4c1a-905f-cd5df37c12af",
   "metadata": {},
   "source": [
    "# Several inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "6276ced8-a115-404c-8cbd-20cae95d138a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df52c1dc-f327-4171-8001-ec0a35f93386",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47244074-3396-4a27-81ec-e84739b9bc2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85db1014-a638-454a-a295-1267d28cac74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24e072f-66ca-4267-81ed-f0d92d380a25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23de17a3-24b0-49e4-9b1f-0403746011b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeecba5-9497-4a15-bb39-6e51a4aa9cc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "185e50af-dd4f-4eea-9202-91eefd7eed71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d727754-c188-48a9-acbf-09749b57c1f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde1e244-0848-4f6a-88fe-6fa373bdc501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9287f23-5c38-4aa0-9f1e-5b686a3dd286",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beb7154-b2e4-43f5-83eb-aef383bdb911",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fe5864-4f23-456a-9049-b3ee28097684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d138748-c209-4680-b76a-5ab220a010a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29750992-4b60-4d45-83fe-74e7eff5d73e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a40d9cf-e1e0-4564-8590-b1418560b2d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f8356c-9cbf-4d68-b59a-a1bbfc346d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ad97626f-66f8-4d07-b274-b33cfd50e196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NWB file saved at nwbdata/harry.nwb!\n"
     ]
    }
   ],
   "source": [
    "nwbfile_path = \"nwbdata/harry.nwb\"  # This should be something like: \"./saved_file.nwb\"\n",
    "interface.run_conversion(nwbfile_path=nwbfile_path, metadata=metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "14298cdf-8ceb-4b30-bfc7-ee109dc6564e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepDict: {'session_description': 'Auto-generated by neuroconv', 'identifier': 'ddbf1f5b-3ff0-4d02-b1ab-0f3cde33d553', 'session_start_time': datetime.datetime(2023, 10, 30, 12, 38, 29, tzinfo=tzfile('/usr/share/zoneinfo/GMT'))}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata['NWBFile']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27893546-b095-4438-9656-0c344324952e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7cc4c91b-7df2-4986-9071-b3ba09436a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dateutil import tz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7eb7d00-ef43-4934-a8c9-d0f588ea95fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SpikeDev",
   "language": "python",
   "name": "spikedev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
