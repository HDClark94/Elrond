{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c31cfcb-7bd4-4f17-9454-18b2a7ad2af3",
   "metadata": {},
   "source": [
    "Hello.\n",
    "\n",
    "Here is code to fix gaps in the files. The functions are from the pipeline, but I'm going to define everything locally in this notebook, so that it's self contained. Here's all the stolen stuff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdbdcc04-2730-4bbe-8e19-e7772b7c0b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "import scipy.io\n",
    "import time\n",
    "import struct\n",
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "# constants\n",
    "NUM_HEADER_BYTES = 1024\n",
    "SAMPLES_PER_RECORD = 1024\n",
    "BYTES_PER_SAMPLE = 2\n",
    "RECORD_SIZE = 4 + 8 + SAMPLES_PER_RECORD * BYTES_PER_SAMPLE + 10 # size of each continuous record in bytes\n",
    "RECORD_MARKER = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 255])\n",
    "\n",
    "# constants for pre-allocating matrices:\n",
    "MAX_NUMBER_OF_SPIKES = int(1e6)\n",
    "MAX_NUMBER_OF_RECORDS = int(1e6)\n",
    "MAX_NUMBER_OF_EVENTS = int(1e6)\n",
    "\n",
    "\n",
    "def readHeader(f):\n",
    "    header = { }\n",
    "    h = f.read(1024).decode().replace('\\n','').replace('header.','')\n",
    "    for i,item in enumerate(h.split(';')):\n",
    "        if '=' in item:\n",
    "            header[item.split(' = ')[0]] = item.split(' = ')[1]\n",
    "    return header\n",
    "\n",
    "\n",
    "def loadContinuous(filepath, dtype = float):\n",
    "\n",
    "    assert dtype in (float, np.int16), \\\n",
    "        'Invalid data type specified for loadContinous, valid types are float and np.int16'\n",
    "\n",
    "    #print(\"Loading continuous data...\")\n",
    "\n",
    "    ch = { }\n",
    "\n",
    "    #read in the data\n",
    "    f = open(filepath,'rb')\n",
    "\n",
    "    fileLength = os.fstat(f.fileno()).st_size\n",
    "\n",
    "    # calculate number of samples\n",
    "    recordBytes = fileLength - NUM_HEADER_BYTES\n",
    "    if  recordBytes % RECORD_SIZE != 0:\n",
    "        raise Exception(\"File size is not consistent with a continuous file: may be corrupt\")\n",
    "    nrec = recordBytes // RECORD_SIZE\n",
    "    nsamp = nrec * SAMPLES_PER_RECORD\n",
    "    # pre-allocate samples\n",
    "    samples = np.zeros(nsamp, dtype)\n",
    "    timestamps = np.zeros(nrec)\n",
    "    recordingNumbers = np.zeros(nrec)\n",
    "    indices = np.arange(0, nsamp + 1, SAMPLES_PER_RECORD, np.dtype(np.int64))\n",
    "\n",
    "    header = readHeader(f)\n",
    "\n",
    "    recIndices = np.arange(0, nrec)\n",
    "\n",
    "    for recordNumber in recIndices:\n",
    "\n",
    "        timestamps[recordNumber] = np.fromfile(f,np.dtype('<i8'),1) # little-endian 64-bit signed integer\n",
    "        N = np.fromfile(f,np.dtype('<u2'),1)[0] # little-endian 16-bit unsigned integer\n",
    "\n",
    "        #print index\n",
    "\n",
    "        if N != SAMPLES_PER_RECORD:\n",
    "            raise Exception('Found corrupted record in block ' + str(recordNumber))\n",
    "\n",
    "        recordingNumbers[recordNumber] = (np.fromfile(f,np.dtype('>u2'),1)) # big-endian 16-bit unsigned integer\n",
    "\n",
    "        if dtype == float: # Convert data to float array and convert bits to voltage.\n",
    "            data = np.fromfile(f,np.dtype('>i2'),N) * float(header['bitVolts']) # big-endian 16-bit signed integer, multiplied by bitVolts\n",
    "        else:  # Keep data in signed 16 bit integer format.\n",
    "            data = np.fromfile(f,np.dtype('>i2'),N)  # big-endian 16-bit signed integer\n",
    "        samples[indices[recordNumber]:indices[recordNumber+1]] = data\n",
    "\n",
    "        marker = f.read(10) # dump\n",
    "\n",
    "    #print recordNumber\n",
    "    #print index\n",
    "\n",
    "    ch['header'] = header\n",
    "    ch['timestamps'] = timestamps\n",
    "    ch['data'] = samples  # OR use downsample(samples,1), to save space\n",
    "    ch['recordingNumber'] = recordingNumbers\n",
    "    f.close()\n",
    "    return ch\n",
    "\n",
    "\n",
    "def writeFrame(f, timestamp, recording_num, x):\n",
    "    byteWritten = 0\n",
    "    if x.size==1024:      \n",
    "        byteWritten += f.write(np.array(timestamp).astype('<i8').tobytes())\n",
    "        byteWritten += f.write(np.array(1024).astype('<i2').tobytes())\n",
    "        byteWritten += f.write(np.array(recording_num).astype('<i2').tobytes())\n",
    "        byteWritten += f.write(x.astype('>i2').tobytes())\n",
    "        byteWritten += f.write(np.array([0,1,2,3,4,5,6,7,8,255]).astype(np.byte).tobytes())\n",
    "    else:\n",
    "        print('Data point not correct. Skipped')\n",
    "    return byteWritten\n",
    "    \n",
    "\n",
    "def writeHeader(f,header):\n",
    "    headerstr=''\n",
    "    for k,v in header.items():\n",
    "        k = 'header.'+k.strip()\n",
    "        headerstr = headerstr+'{0} = {1};\\n'.format(k,v)\n",
    "    headerstr=headerstr.ljust(1024)\n",
    "    f.write(headerstr.encode('ascii'))\n",
    "\n",
    "def writeContinuousFile(fname,header,timestamp,x,recording_num=None,dtype=np.float64):\n",
    "    f = open(fname,'wb')\n",
    "    writeHeader(f,header)\n",
    "\n",
    "    noFrame = x.size//1024\n",
    "    \n",
    "    if dtype == np.float64:\n",
    "        #convert back the value to int according to the bitVolts\n",
    "        x = np.round(x/np.float64(header['bitVolts']))\n",
    "    \n",
    "    for i in range(noFrame):\n",
    "        if recording_num is not None:\n",
    "            writeFrame(f,timestamp[i],recording_num[i],x[i*1024:(i+1)*1024])\n",
    "        else:\n",
    "            writeFrame(f,timestamp[i],0,x[i*1024:(i+1)*1024])\n",
    "    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb5366e-290e-4bc7-9fcf-94292c98e3bf",
   "metadata": {},
   "source": [
    "And own functions ( I can write code too! )\n",
    "\n",
    "This function has two purposes: it can find gaps and it can delete them.\n",
    "\n",
    "Note: We need to search for the gaps. But sometimes the recording is 0 for a few samples (biggest I've seen was 4). So I define a gap as being at least 10 zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6803b30-e7f3-4573-9958-f7c9eb457a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "how_many_zeros_is_a_gap = 10\n",
    "\n",
    "def delete_blanks(raw_data):\n",
    "\n",
    "    locations = []\n",
    "\n",
    "    new_raw_data = raw_data\n",
    "\n",
    "    zero_locations = np.nonzero((new_raw_data == 0)*1)[0]\n",
    "\n",
    "    j = 0\n",
    "    while j < len(zero_locations):\n",
    "        i = zero_locations[j]\n",
    "        count = 1\n",
    "        while (count < len(new_raw_data)-i) and new_raw_data[i+count] == 0.:\n",
    "            count = count + 1\n",
    "        if count > how_many_zeros_is_a_gap:\n",
    "            # if you wanna see where the gap is.\n",
    "            # print(i, count)\n",
    "            new_raw_data = np.delete(new_raw_data, np.s_[i:i+count])\n",
    "            zero_locations = np.nonzero((new_raw_data == 0)*1)[0]\n",
    "            locations.append(i)\n",
    "        else:\n",
    "            j = j + count\n",
    "        count = 1\n",
    "\n",
    "    return new_raw_data, locations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63b1948-a72b-49c2-a809-046d5b474545",
   "metadata": {},
   "source": [
    "Here's the list of bad files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0f6d17-8a56-4aec-b881-2c4eb53f0406",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_files = [\n",
    "'cohort8_may2021/vr/M10_D15_2021-05-28_10-06-58',\n",
    "'cohort8_may2021/vr/M15_D27_2021-06-15_13-06-29',\n",
    "'cohort8_may2021/vr/M15_D13_2021-05-26_12-05-34',\n",
    "'cohort8_may2021/vr/M15_D16_2021-05-31_12-40-03',\n",
    "'cohort8_may2021/vr/M13_D29_2021-06-17_11-50-37',\n",
    "'cohort8_may2021/vr/M14_D41_2021-07-05_12-37-48',\n",
    "'cohort8_may2021/vr/M14_D41_2021-07-05_12-37-48',\n",
    "'cohort8_may2021/vr/M13_D18_2021-06-02_11-50-48',\n",
    "'cohort8_may2021/vr/M11_D29_2021-06-17_10-35-48',\n",
    "'cohort8_may2021/vr/M10_D16_2021-05-31_09-42-48',\n",
    "'cohort8_may2021/vr/M12_D7_2021-05-18_10-28-40',\n",
    "'cohort8_may2021/vr/M15_D9_2021-05-20_12-37-07',\n",
    "'cohort8_may2021/vr/M12_D24_2021-06-10_11-24-37',\n",
    "'cohort8_may2021/vr/M14_D37_2021-06-29_12-33-24',\n",
    "'cohort8_may2021/vr/M14_D33_2021-06-23_12-22-49',\n",
    "'cohort8_may2021/vr/M11_D14_2021-05-27_10-34-15',\n",
    "'cohort8_may2021/vr/M11_D17_2021-06-01_10-36-53',\n",
    "'cohort8_may2021/vr/M13_D11_2021-05-24_11-11-59',\n",
    "'cohort8_may2021/of/M14_D13_2021-05-26_10-51-36',\n",
    "'cohort8_may2021/of/M12_D29_2021-06-17_10-31-00',\n",
    "'cohort7_october2020/vr/M4_D15_2020-11-16_14-59-09',\n",
    "'cohort7_october2020/vr/M3_D30_2020-12-07_14-55-41',\n",
    "'cohort7_october2020/vr/M3_D19_2020-11-22_14-48-51',\n",
    "'cohort7_october2020/vr/M4_D1_2020-10-29_14-21-55',\n",
    "'cohort7_october2020/vr/M7_D20_2020-11-23_15-47-55',\n",
    "'cohort7_october2020/vr/M4_D9_2020-11-08_15-18-01',\n",
    "'cohort7_october2020/vr/M4_D12_2020-11-13_15-08-51',\n",
    "'cohort7_october2020/vr/M3_D22_2020-11-27_15-01-24',\n",
    "'cohort7_october2020/vr/M3_D18_2020-11-21_14-29-49',\n",
    "'cohort7_october2020/vr/M4_D7_2020-11-06_14-58-44',\n",
    "'cohort7_october2020/vr/M4_D2_2020-10-30_15-23-49',\n",
    "'cohort7_october2020/vr/M3_D25_2020-11-30_15-13-15',\n",
    "'cohort7_october2020/vr/M6_D8_2020-11-07_15-49-19',\n",
    "'cohort7_october2020/vr/M6_D9_2020-11-08_15-53-11',\n",
    "'cohort7_october2020/vr/M4_D14_2020-11-15_14-58-40',\n",
    "'cohort7_october2020/vr/M6_D1_2020-10-29_15-04-09',\n",
    "'cohort7_october2020/vr/M6_D13_2020-11-14_15-40-42',\n",
    "'cohort7_october2020/vr/M7_D28_2020-12-05_16-15-10',\n",
    "'cohort7_october2020/vr/M3_D6_2020-11-05_14-37-17',\n",
    "'cohort7_october2020/vr/M4_D23_2020-11-28_15-45-12',\n",
    "'cohort7_october2020/vr/M4_D8_2020-11-07_15-13-58',\n",
    "'cohort7_october2020/vr/M4_D20_2020-11-23_15-08-48',\n",
    "'cohort7_october2020/vr/M3_D4_2020-11-01_14-12-26',\n",
    "'cohort7_october2020/vr/M7_D5_2020-11-02_15-52-51',\n",
    "'cohort7_october2020/vr/M7_D8_2020-11-07_16-24-43',\n",
    "'cohort7_october2020/vr/M4_D10_2020-11-09_15-04-26',\n",
    "'cohort7_october2020/vr/M6_D26_2020-12-03_16-42-12',\n",
    "'cohort7_october2020/vr/M3_D33_2020-12-12_15-02-16',\n",
    "'cohort7_october2020/vr/M6_D11_2020-11-12_15-39-10',\n",
    "'cohort7_october2020/vr/M4_D22_2020-11-27_15-38-49',\n",
    "'cohort7_october2020/vr/M4_D3_2020-10-31_14-46-29',\n",
    "'cohort7_october2020/vr/M4_D28_2020-12-05_15-44-27',\n",
    "'cohort7_october2020/vr/M6_D12_2020-11-13_15-44-22',\n",
    "'cohort7_october2020/of/M6_D8_2020-11-07_16-23-00',\n",
    "'cohort6_july2020/vr/M1_D9_2020-08-13_15-16-48',\n",
    "'cohort6_july2020/vr/M1_D6_2020-08-10_14-17-21'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8717e37a-86fc-4cee-862c-a3ff7c31e365",
   "metadata": {},
   "source": [
    "First, we can check for gaps. I've already run this, so you don't have to!\n",
    "\n",
    "Another assumption here: I've just looked at the CH1 file, assuming that any gap the same for all channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9697e75-557d-47d4-a47b-bb103d9ab42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point it to the datastore, however you do this\n",
    "oe_folder = Path(\"/Volumes/cmvm/sbms/groups/CDBS_SIDB_storage/NolanLab/ActiveProjects/Harry\")\n",
    "\n",
    "for bad_file in bad_files:\n",
    "    \n",
    "    data_dict = loadContinuous(oe_folder / bad_file / '100_CH1.continuous', dtype=np.int16)\n",
    "\n",
    "    print(f\"{bad_file}, \", end=\"\")\n",
    "    \n",
    "    raw_data = data_dict['data']\n",
    "    _, locs = delete_blanks(data_dict['data'])\n",
    "\n",
    "    if len(locs) > 0:\n",
    "    \n",
    "        if locs[0] + 1024 == len(raw_data):\n",
    "            print(\"just has a blank chunk at the end.\", end=\"\")\n",
    "        else:\n",
    "            print(f\"len = {len(raw_data)/30000/60} \", end=\"\")\n",
    "            for loc in locs:\n",
    "                if loc + 1024 == len(raw_data):\n",
    "                    print(\"and a blank chunk at the end.\", end=\"\")\n",
    "                else:\n",
    "                    print(f\", gap = {(loc)/30000/60}\", end=\"\")\n",
    "        print()\n",
    "\n",
    "    else:\n",
    "\n",
    "        print(\"no gaps!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c0af8d-7e74-4e64-b993-f972366c5e11",
   "metadata": {},
   "source": [
    "I found the following bad files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "671ead1e-fa71-4d39-9fb1-2d9f08333458",
   "metadata": {},
   "outputs": [],
   "source": [
    "actually_bad_files = [\n",
    "'cohort8_may2021/vr/M13_D29_2021-06-17_11-50-37',\n",
    "'cohort8_may2021/of/M12_D29_2021-06-17_10-31-00',\n",
    "'cohort7_october2020/vr/M3_D6_2020-11-05_14-37-17',\n",
    "'cohort6_july2020/vr/M1_D9_2020-08-13_15-16-48',\n",
    "'cohort6_july2020/vr/M1_D6_2020-08-10_14-17-21',\n",
    "'cohort8_may2021/of/M14_D13_2021-05-26_10-51-36'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f6907a-6c55-4fb9-bac0-28e1d2cc36d2",
   "metadata": {},
   "source": [
    "For these ones, we can use the same function to get rid of the chunks, with a 'lil modification\n",
    "\n",
    "In this code, we create a new folder with the same name but with \"_fixed\" appended to the name. You can do whatever you want. I, obviously, wouldn't delete the original data yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "14521621-836d-4db1-a5cb-444f191fc2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2n/729_xkd94b7d1rc0nc9t3jsh0000gp/T/ipykernel_60774/1521856135.py:68: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  timestamps[recordNumber] = np.fromfile(f,np.dtype('<i8'),1) # little-endian 64-bit signed integer\n",
      "/var/folders/2n/729_xkd94b7d1rc0nc9t3jsh0000gp/T/ipykernel_60774/1521856135.py:76: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recordingNumbers[recordNumber] = (np.fromfile(f,np.dtype('>u2'),1)) # big-endian 16-bit unsigned integer\n"
     ]
    }
   ],
   "source": [
    "# A little local test for me\n",
    "#\n",
    "# oe_folder = Path('/Users/chris/Work/Edinburgh/Spike/data/Harry/')\n",
    "# actually_bad_files = ['M1_D6_2020-08-10_14-17-21']\n",
    "\n",
    "for in_folder in actually_bad_files:\n",
    "\n",
    "    out_folder = in_folder + '_fixed'\n",
    "    if not os.path.isdir(oe_folder / out_folder):\n",
    "        os.mkdir(oe_folder / out_folder )\n",
    "\n",
    "    for a in range(1,3):\n",
    "\n",
    "        filename = \"100_CH\" + str(a) + \".continuous\"\n",
    "\n",
    "        data_dict = loadContinuous(oe_folder / in_folder / filename, dtype = np.int16)\n",
    "\n",
    "        raw_data = data_dict['data']\n",
    "        new_raw_data, _ = delete_blanks(raw_data)\n",
    "\n",
    "        timestamps = data_dict['timestamps']\n",
    "        new_timestamps = (timestamps[0] + np.arange(0,len(new_raw_data)+1024,1024))\n",
    "\n",
    "        f = open(oe_folder / in_folder / filename, 'rb')\n",
    "        header = readHeader(f)\n",
    "\n",
    "        writeContinuousFile(oe_folder / out_folder / filename, header, new_timestamps, new_raw_data, dtype=np.int16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b8da18-cfa3-4ca7-8389-599976500c0c",
   "metadata": {},
   "source": [
    "We can re-check the fixed data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "577cea7a-f1a9-4ec5-ac15-9163ce04daa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2n/729_xkd94b7d1rc0nc9t3jsh0000gp/T/ipykernel_60774/1521856135.py:68: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  timestamps[recordNumber] = np.fromfile(f,np.dtype('<i8'),1) # little-endian 64-bit signed integer\n",
      "/var/folders/2n/729_xkd94b7d1rc0nc9t3jsh0000gp/T/ipykernel_60774/1521856135.py:76: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  recordingNumbers[recordNumber] = (np.fromfile(f,np.dtype('>u2'),1)) # big-endian 16-bit unsigned integer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M1_D6_2020-08-10_14-17-21_fixed, no gaps!\n"
     ]
    }
   ],
   "source": [
    "# A little local test for me\n",
    "#\n",
    "# oe_folder = Path('/Users/chris/Work/Edinburgh/Spike/data/Harry/')\n",
    "# bad_files = ['M1_D6_2020-08-10_14-17-21_fixed']\n",
    "\n",
    "for bad_file in bad_files:\n",
    "\n",
    "    data_dict = loadContinuous(oe_folder / bad_file / '100_CH1.continuous', dtype=np.int16)\n",
    "\n",
    "    print(f\"{bad_file}, \", end=\"\")\n",
    "\n",
    "    raw_data = data_dict['data']\n",
    "    _, locs = delete_blanks(data_dict['data'])\n",
    "\n",
    "    if len(locs) > 0:\n",
    "\n",
    "        if locs[0] + 1024 == len(raw_data):\n",
    "            print(\"just has a blank chunk at the end.\", end=\"\")\n",
    "        else:\n",
    "            print(f\"len = {len(raw_data)/30000/60} \", end=\"\")\n",
    "            for loc in locs:\n",
    "                if loc + 1024 == len(raw_data):\n",
    "                    print(\"and a blank chunk at the end.\", end=\"\")\n",
    "                else:\n",
    "                    print(f\", gap = {(loc)/30000/60}\", end=\"\")\n",
    "        print()\n",
    "\n",
    "    else:\n",
    "        \n",
    "        print(\"no gaps!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4494353-c69a-45be-9121-672df58a5b49",
   "metadata": {},
   "source": [
    "GOOD TIMES!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0493e3-381f-4112-8e12-ec41bb79c46c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SpikeDev",
   "language": "python",
   "name": "spikedev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
